{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by [Julia Ive](mailto:j.ive@imperial.ac.uk), [Zhenhao Li](mailto:zhenhao.li18@imperial.ac.uk), and [Nihir](mailto:nv419@ic.ac.uk).\n",
    "\n",
    " [The AI Core](https://theaicore.com)\n",
    " \n",
    "Download GloVe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [RNNs Recap and Primer](#RNNs-Recap-and-Primer)\n",
    " - [RNNs for text classification](#RNNs-for-Text-Classification-Task)\n",
    " - [Bi-directional RNNs](#BiDirectional-RNNs)\n",
    "- [RNNs for Language Modelling](#RNNs-for-Language-Modelling)\n",
    " - [Evaluation of Language Models](#Evaluation-of-Language-Models)\n",
    " - [Long short term memory architectures LSTMs vs. RNNs](#Long-short-term-memory-architectures-LSTMs-vs.-RNNs)\n",
    "- [Sequence to sequence model](#Sequence-to-sequence-model)\n",
    " - [BLEU Score](#BLEU-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "RNNs are designed to make use of sequential data, when the current step has some kind of relation with the previous steps. This makes them ideal for applications with a time component (audio, time-series data) and natural language. RNNs are networks for which value of a unit depends on its own previous output as input.\n",
    "\n",
    "![](images/RNN_basic.JPG)\n",
    "\n",
    "\n",
    "An input vector representing the current input element $x_t$ is multiplied by a weight matrix $W$ and then passed through an activation function to compute an activation value for a layer of hidden units. This hidden layer is, in turn, used to calculate a corresponding output, $y_t$. \n",
    "\n",
    "$h_t = g(Uh_{t-1}+Wx_t)$\n",
    "\n",
    "$y_t = a(Vh_t)$\n",
    "\n",
    "The hidden layer from the previous time step $h_{t-1}$ provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. $U$ determine how the network should make use of past context. RNNs do not impose any limit on this prior context. The context includes information dating back to the beginning of the sequence. Three sets of weights are updated at each timestep: $W$, $U$ and $V$.\n",
    "\n",
    "\n",
    "![rnn_cell](images/rnn_cell.png)\n",
    "\n",
    "## Optimization\n",
    "\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "![](images/RNN_BPTT.JPG)\n",
    "\n",
    "\n",
    "## RNNs for Text Classification Task\n",
    "\n",
    "There are many variations of RNN’s likely Many-One, Many-Many, etc. In our case we aim to classify the input text into positive and negative class. So, set of Many-One LSTM units achieves the task as only one value needs to be outputted for determining the polarity of the review. Usually this is the last RNN hidden state as the one that summarises the whole sequence.\n",
    "\n",
    "\n",
    "Q: Why RNNs better than FFNNs?\n",
    "\n",
    "FFNNs do not take context into account. Each word is represented by its embedding independent of other words. RNNs encodes each new word (token) considering the previous words. Meaning of words can change depending on the context. For example, compare the meanings of the word \"mean\" in those two sentences \"I compute the mean\" and \"His behaviour was mean\".\n",
    "\n",
    "Q: How the model is evaluated ?\n",
    "\n",
    "The natural choise is accuracy - \n",
    "percentage of all the examples that our model labeled correctly. However, it is not good for unbalanced datasets. Imagine a dataset with 999,900 positive examples and 100 negative examples. A very bad classifier can assign positive class to all the examples. This classifier would have 999,900 true negatives and only 100 false negatives and an accuracy of\n",
    "999,900/1,000,000 or 99.99%! \n",
    "\n",
    "Other metrics, more useful for such datasets are: precision, recall and F-measure. Precision measures the percentage of the items that the system labelled as positive and are positive accroding to the gold labels. Recall measures the percentage of items labelled as positive out of all the gold positive items. F-score is the weighted harmonic mean of the precision and recall. \n",
    "\n",
    "\n",
    "We will work with a popular classification task of sentiment analysis, the extraction of the sentiment that a writer expresses toward something he/she describes.\n",
    "\n",
    "![rnn_classification](images/rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "#We will work with a dataset from the torchtext package consists of data processing utilities and popular datasets for NLP\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# We fix the seeds to get consistent results for each training.\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Helper function to print time between epochs\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With TorchText Field we define how our data will be processed: here we will use Spacy for tokenisation\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "# We will experiment with a widely used Stanford Treebank dataset and will predict sentiment of movie reviews\n",
    "# Our data will be classified in three labels: positive, negative and neutral\n",
    "# We take the standard split\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SST.splits(\n",
    "            TEXT, LABEL)\n",
    "\n",
    "# Print stat over the data\n",
    "\n",
    "print('train.fields:', train_data.fields)\n",
    "print('len(train):', len(train_data))\n",
    "print('vars(train[0]):', vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a vocabulary out of tokens available from the pre-trained embedding list and the vocabulary of labels.\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.50d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "print (\"Label Vocabulary Length: \", len(LABEL.vocab))\n",
    "\n",
    "#We can display the most common words in the vocabulary and their frequencies\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "#We can also see the vocabulary directly using the stoi (string to int)\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "# BucketIterator is an iterator that will return a batch of examples of similar lengths, minimizing the amount of padding per example.\n",
    "# Padding refers to fixing the length of inputs (adding a reserved token a certain amount of times to match certain length), usually to the max length within a batch. For exmaple:\n",
    "# i         like  this  movie <pad>\n",
    "# the       movie is    very  good\n",
    "# excellent !     <pad> <pad> <pad>\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "            batch_size = BATCH_SIZE,\n",
    "            sort_within_batch = True,\n",
    "            device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_iterator)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    demo_batch = batch\n",
    "    break\n",
    "    \n",
    "print(demo_batch)\n",
    "\n",
    "print()\n",
    "\n",
    "# Note that demo_batch.text has a shape of [sentence length x batch size]\n",
    "print(\"Demo batch `text` shape:\", demo_batch.text.shape)\n",
    "# We can simply reshape this into the more familiar [batch size x sentence length]\n",
    "print(\"Demo batch `text` transpose shape:\", demo_batch.text.T.shape)\n",
    "print(\"Demo batch `text` sample: \\n\", demo_batch.text.T[:3, :])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Demo batch `label` shape:\", demo_batch.label.shape)\n",
    "# shape(demo_batch.label.shape) = [batch size]\n",
    "print(\"Demo batch `label` sample:\", demo_batch.label[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RNN class\n",
    "\n",
    "Within the constructor we define the layers:\n",
    " - An **embedding layer** which acts as a lookup table to map our tokens to their vector \n",
    " - An **RNN** \n",
    " - A **linear layer**. This layer receives the last hidden state from the RNN and outputs logits of `output dim` dimensionality \n",
    "\n",
    "All the parameters initialized to random values by default, unless explicitly specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # An embedding layer (look-up layer) transforms word indicies into word embeddings. \n",
    "        # Here, we initialize our model with pre-trained embeddings (100D pre-trained GloVe embeddings in our case).\n",
    "        # This layer will fine-tune these embeddings, specific to this model/dataset.\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        # We can also train the embeddings from scratch:\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # An RNN layer. we specify that the batch dimension goes first\n",
    "        # We have a bidirectional flag which indicates whether the model is unidirectional or bidirectional\n",
    "        # RNNs can be stacked - i.e. have multiple layers. Here, we will only look at the 1 layer case.\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          num_layers=1)\n",
    "\n",
    "        # The linear layer takes the final hidden state and feeds it through a fully connected layer.\n",
    "          # The dimensionality of the output is equal to the output class count.\n",
    "          # For classification in a bidirectional RNN we concatenate:\n",
    "            #  - The last hidden state from the forward RNN (obtained from final word of the sentence)\n",
    "            #  - The last hidden state from the backward RNN (obtained from the first word of the sentence)\n",
    "          # Due to the concatenation, our hidden size is doubled.\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            linear_hidden_in = hidden_dim * 2\n",
    "        else:\n",
    "            linear_hidden_in = hidden_dim\n",
    "\n",
    "        # The classification (linear) layer\n",
    "        self.fc = nn.Linear(linear_hidden_in, output_dim)\n",
    "\n",
    "        # We apply dropout technique that sets a random set of activations in the max-pooling layer to zero.\n",
    "          # This prevents the network from learning to rely on specific weights and helps to prevent overfitting. \n",
    "          # Note that the dropout layer is only used during training, and not during test time.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # ACRONYMS:\n",
    "          # B = Batch size\n",
    "          # T = Max sentence length\n",
    "          # E = Embedding dimension\n",
    "          # D = Hidden dimension\n",
    "          # O = Output dimension\n",
    "\n",
    "        # shape(text) = [B, T]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # shape(embedded) = [B, T, E]\n",
    "        \n",
    "        # An RNN in PyTorch returns two values:\n",
    "        # (1) All hidden states of the last RNN layer\n",
    "        # (2) Hidden state of the last timestep for every layer\n",
    "          # Note: we are only using 1 layer\n",
    "        all_hidden, last_hidden = self.rnn(embedded)\n",
    "        # shape(all_hidden) = [B, T, D*num_directions]\n",
    "        # shape(last_hidden) = [num_layers*num_directions, B, D].  num_layers = 1\n",
    "        # NOTE. If we were to NOT use the `batch_first` flag, shape of all_hidden would be [T, B, D*num_directions]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n",
    "            last_hidden = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=-1)\n",
    "            # shape(last_hidden) = [B, D*2]\n",
    "\n",
    "        else:\n",
    "            last_hidden = last_hidden.squeeze(0)\n",
    "            # shape(last_hidden) = [B, D]\n",
    "\n",
    "        # Our predictions.\n",
    "        logits = self.fc(self.dropout(last_hidden))\n",
    "        # shape(logits) = [B, O]\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.3\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise the RNN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    returns accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    class_preds = nn.Softmax(dim=-1)(preds)\n",
    "    class_preds = class_preds.max(-1)[1]\n",
    "    correct = (class_preds == y).float() # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizier, criterion, N_EPOCHS=15):\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "\n",
    "        # To ensure the dropout is \"turned on\" while training\n",
    "          # (good practice to include in your projects even if it is not used)\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        # `batch` is a tuple of Tensors: (TEXT, LABEL)\n",
    "        for batch in train_iterator:\n",
    "                        \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "            # shape(text) = [T, B]\n",
    "            # shape(label) = [B]\n",
    "            \n",
    "            # We reshape text to [B, T]. \n",
    "            # This is purely so we can think about the shapes of the Tensors more consistently\n",
    "            text = text.T\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "        \n",
    "            # compute training accuracy\n",
    "            acc = accuracy(predictions, labels)\n",
    "              \n",
    "            # calculate the gradient of each parameter\n",
    "            loss.backward()\n",
    "        \n",
    "            # update the parameters using the gradients and optimizer algorithm \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        average_epoch_loss = epoch_loss / len(train_iterator)\n",
    "        average_epoch_acc = epoch_acc / len(train_iterator)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        average_epoch_valid_loss, average_epoch_valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {average_epoch_loss:.3f} | Train Acc: {average_epoch_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {average_epoch_valid_loss:.3f} |  Val. Acc: {average_epoch_valid_acc*100:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # Turn on evaluate mode.\n",
    "      # De-activates dropout and batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # We do not compute gradients within this block, i.e. no training\n",
    "    # https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "\n",
    "            text = text.T\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs at the same step are then usually concatenated. This can provide additional useful context to the model.\n",
    "\n",
    "Q: Why is a bi-directional RNN is better than single-direction ?\n",
    "\n",
    "Imagine that you see only the left context: \"We went to ...\" This context is very general and a lot of different words can continue: nouns (London, work, cinema, doctor), verbs (join, support), etc. When we both left and right contexts the word \"sleep\" is becoming evident: \"We went to ... early but still could not wake up on time.\"\n",
    "\n",
    "\n",
    "![bi_rnn_classification](images/bi_rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDIRECTIONAL = True\n",
    "\n",
    "bidirectional_model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(bidirectional_model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(bidirectional_model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for Language Modelling\n",
    "\n",
    "Language model is required to represent the text to a form understandable from the machine point of view. Language Modelling (LM) is at the core of Natural Language Processing (NLP). Base of all the NLP tasks: Machine Translation, Spell Correction, Speech Recognition, Summarization, Question Answering, Sentiment analysis etc.  \n",
    "\n",
    "Language is a sequence of letters, words, sentences, paragraphs, etc. These units are not independent. When we comprehend and produce spoken language, we are processing continuous input streams of indefinite\n",
    "length. And even when dealing with written text we normally read it sequentially. Thus, RNNs is a perfect fit to model language data because with RNNs we can represent language sequence of any length into a fixed-sized vector.\n",
    " \n",
    "Q: What is the difference between word embeddings and language modelling?\n",
    "\n",
    "The main difference that word embeddings do not take into account word order. Language models take word order into account. The word order is important. If you do not take the word order into account the representation of the following sentences will be the same: \"It was really not good, on the opposite quite bad.\" and \"It was really not bad, on the opposite quite good.\" However, the meaning of those two sentences is different.\n",
    "\n",
    "![rnn_lm](images/rnn_lm.png)\n",
    "\n",
    "\n",
    "You may have heard about BERT. BERT is a general-purpose pre-trained language model. It is pre-trained using a lot of language data from Internet to create a better \"grasp\" of language. It is bidirectional. This means a deeper sense of language context and flow compared to the single-direction language models. You can download it and fine-tune for your NLP problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With TorchText Field we define how our data will be processed\n",
    "TEXT = data.Field(tokenize = 'spacy', init_token = '<sos>')\n",
    "\n",
    "# We will be using the WikiText-2 corpus, which is a popular LM dataset.\n",
    "# The WikiText language modeling dataset is a collection of texts extracted \n",
    "  # from good and featured articles on Wikipedia.\n",
    "# It contains about 2 million words \n",
    "train_data, valid_data, test_data = datasets.WikiText2.splits(TEXT)\n",
    "\n",
    "# Data stats\n",
    "print('train.fields', train_data.fields)\n",
    "print('len(train)', len(train_data))\n",
    "\n",
    "# Build a vocabulary out of tokens available from the pre-trained embeddings list and the vocabulary of labels\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of the Backpropagation training algorithm to RNNs applied to sequence data. Each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPTTIterator (Backpropagation Through Time Iterator)\n",
    "# divides the corpus into batches of [batch size, bptt_len/sequence length]\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BPTTIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "                batch_size = BATCH_SIZE, bptt_len=30,\n",
    "                device = device, repeat=False)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    demo_batch = batch\n",
    "    break\n",
    "    \n",
    "print(demo_batch)\n",
    "\n",
    "print()\n",
    "\n",
    "# Note that the first dimension is the sequence, and the next is the batch.\n",
    "  # We can reshape this to [batch size, sentence length] as we did earlier with a transpose.\n",
    "# The target is the original text offset by 1\n",
    "print(\"Demo batch `text`:\\n\", demo_batch.text[:5, :3])\n",
    "print(\"Demo batch `target`:\\n\", demo_batch.target[:5, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    # variant is a flag which is either: \"rnn\", \"lstm\", \"manual_lstm\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx, variant):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.variant = variant\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "\n",
    "        # UNIDIRECTIONAL RNN layer: For LM modelling we do not see/have access to the right context\n",
    "        \n",
    "        if variant == \"rnn\":\n",
    "            self.rnn = nn.RNN(embedding_dim, \n",
    "                               hidden_dim, \n",
    "                               batch_first=True)\n",
    "        elif variant == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, \n",
    "                               hidden_dim, \n",
    "                               batch_first=True)\n",
    "        elif variant == \"manual_lstm\":\n",
    "            self.rnn = Manual_LSTM(embedding_dim, hidden_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Expected `variant` to be one of 'rnn', 'lstm', or 'manual_lstm'\")\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "       \n",
    "    def forward(self, text, prev_hidden):\n",
    "         \n",
    "        # shape(text) = [B, T]\n",
    "        \n",
    "        # If vanilla RNN:\n",
    "            # shape(prev_hidden) = [1, B, D] where 1 = num_layers*num_directions\n",
    "        # If LSTM:\n",
    "            # prev_hidden is a tuple of previous hidden states and cell states: (ALL_HIDDEN_STATES, ALL_CELL_STATES)\n",
    "            # shape(ALL_HIDDEN_STATES)=shape(ALL_CELL_STATES) = [1, B, D] where 1 = num_layers*num_directions\n",
    "            \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # shape(embedded) = [B, T, E]\n",
    "        \n",
    "        all_hidden, last_hidden = self.rnn(embedded, prev_hidden)        \n",
    "        # shape(all_hidden) = [B, T, D]\n",
    "        # shape(last_hidden) = [num layers, B, T]\n",
    "        \n",
    "        # Take all hidden states to produce an output word per time step\n",
    "        logits = self.fc(self.dropout(all_hidden))\n",
    "        # shape(logits) = [B, O]\n",
    "            \n",
    "        return logits, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(TEXT.vocab)\n",
    "DROPOUT = 0.5\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters())\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to detach the hidden state or else the model will try to backpropagate to the beginning of the dataset, requiring a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden(hidden):\n",
    "  \"\"\"Wraps hidden states in new Tensors, to declare it not to need gradients. So that the initial hidden state for this batch is constant and doesn’t depend on anything.\"\"\"\n",
    "\n",
    "  if isinstance(hidden, torch.Tensor):\n",
    "    return hidden.detach()\n",
    "  else:\n",
    "    return tuple(save_hidden(v) for v in hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Language Models\n",
    "\n",
    "Language is very difficult to evaluate since there is no single gold truth: one meaning could be expressed in many valid ways.\n",
    "\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "Human evaluation is costly, slow and subjective but reliable. Human evaluation of a language model may involve how a hypothesis satisfies the grammatical and lexical norms of a language.\n",
    "\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Does it prefer real (=frequently observed) sentences to ‘ungrammatical/gibberish’ (or rarely observed) ones? \n",
    "Remember that entropy is the average number of bits to encode the information contained in a random variable, so the exponentiation of the entropy (perplexity, $e^{H}$) should be the total amount of all possible information, or more precisely, the weighted average number of choices a random variable has. We evaluate our prediction Q by testing against samples drawn from P: $PPL = e^{H(P,Q)}$.\n",
    "\n",
    "Measure perplexity on an unseen (test) corpus, generally we compare a range of models using this score. The best LM is the one that generates the lowest perplexity on the test corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(loss_per_token):\n",
    "    return math.exp(loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion, N_EPOCHS=10, is_lstm=False, force_stop=False):\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_items = 0\n",
    "        \n",
    "        # The `1` is the number of layers * number of directions.\n",
    "        # i.e. we have 1 layer and we are moving in 1 direction\n",
    "        # More info: https://pytorch.org/docs/stable/nn.html#rnn\n",
    "        prev_hidden = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "        if is_lstm:\n",
    "            prev_ht = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "            prev_ct = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "            prev_hidden = (prev_ht, prev_ct)\n",
    "\n",
    "        \n",
    "        # `batch` is a tuple of Tensors: (TEXT, TARGET)\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            \n",
    "            if force_stop:\n",
    "                print(\"Currently processing train batch {} of {}\".format(i, len(train_iterator)))\n",
    "                if i % 7 == 0 and i != 0:\n",
    "                    break\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text = batch.text\n",
    "            targets = batch.target\n",
    "            # shape(text) = [T, B]\n",
    "            # shape(target) = [T, B]\n",
    "            \n",
    "            # We reshape text and target to [B, T]. \n",
    "            text = text.T\n",
    "            targets = targets.T\n",
    "            # shape(text) = [B, T]\n",
    "            # shape(target) = [B, T]\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # Otherwise the model would backpropagate all the way to beginning of the dataset.\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "            \n",
    "            ## Run the model\n",
    "            \n",
    "            \n",
    "            # Compute the loss\n",
    "            # We reshape inputs to eliminate batching\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), targets.reshape(-1))\n",
    "        \n",
    "            ## Backprop and perform gradient descent\n",
    "            \n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "        \n",
    "        # We compute loss per token for an epoch\n",
    "        train_loss_per_token = epoch_loss / epoch_items\n",
    "        # We compute perplexity\n",
    "        train_ppl = perplexity(train_loss_per_token)\n",
    "\n",
    "        end_time = time.time()        \n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        valid_loss_per_token, valid_ppl = evaluate(model, \n",
    "                                                   valid_iterator, \n",
    "                                                   criterion,\n",
    "                                                   is_lstm=is_lstm,\n",
    "                                                   force_stop=force_stop)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss_per_token:.3f} | Train Perplexity: {train_ppl:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss_per_token:.3f} |  Val. Perplexity: {valid_ppl:.3f}')\n",
    "        \n",
    "        if force_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, is_lstm=False, force_stop=False):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "    \n",
    "    # we initialise the first hidden state with zeros\n",
    "    ## Initialise the previous hidden states of the RNNs\n",
    "    ##prev_hidden = \n",
    "#     if is_lstm:\n",
    "        ##prev_ht =\n",
    "        ##prev_ct = \n",
    "        ##prev_hidden = \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            if force_stop and i % 3 == 0 and i != 0:\n",
    "                print(\"Currently processing valid batch {} of {}\".format(i, len(train_iterator)))\n",
    "                break\n",
    "\n",
    "            text, target = batch.text, batch.target\n",
    "            text, target = text.T, target.T\n",
    "            logits, prev_hidden = model(text, prev_hidden)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    "\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "\n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "\n",
    "        loss_per_token = epoch_loss / epoch_items\n",
    "        ppl = math.exp(loss_per_token)\n",
    "            \n",
    "        \n",
    "    return loss_per_token, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(rnn_model, train_iterator, valid_iterator, optimizer, criterion, force_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Long short-term memory architectures LSTMs vs. RNNs\n",
    "\n",
    "**The vanishing gradient problem**. The gradient signal gets smaller and smaller as it backpropagates further. It is caused by the repeated use of the recurrent weight matrix in RNN. Gradient can be viewed as a measure of the effect of the past on the future. If the gradient becomes vanishingly small over longer distances we can not capture the dependency to the past correctly. For example: \"A patient with a rare sarcoma of soft tissue on the left thigh was presented to the hospital yesterday.\" \"was presented\" depends on \"a patient\", but they are separated by 11 words!\n",
    "\n",
    "![full_lstm](images/lstm_full.png)\n",
    "\n",
    "The key to LSTMs is the cell state $c_t$. It runs straight down the entire chain and allow the information to just flow along it unchanged. LSTM has two \"hidden states\": $c_t$  and $h_t$ . You can think of $c_t$  is the \"internal\" hidden state that retains important information for longer timesteps, whereas $h_t$ is the \"external\" hidden state that exposes that information to the outside world.\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  An LSTM has three of these gates.\n",
    "\n",
    "Forget gate decides what information we’re going to throw away from the cell state. \n",
    "\n",
    "$f_t = \\sigma(W_{if}x_t + W_{hf}h_{t-1}+b_f)$\n",
    "\n",
    "$\\sigma$ squashes input values between 0 and 1, describing how much of each component should be let through. Zero means \"let nothing through\", while a value of one means \"let everything through\".\n",
    "\n",
    "![lstm_ft](images/lstm_ft.png)\n",
    "\n",
    "Input gate decides what new information we are going to store in the cell state. \n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + W_{hi}h_{t-1}+b_i)$\n",
    "\n",
    "Next, a tanh layer creates a vector of new candidate values, $g_t$, that could be added to the state. tanh squashes the output values to be between −1 and 1. \n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + W_{hg}h_{t-1}+b_g)$ (this equation equal to vanilla RNN if we remove gates)\n",
    "\n",
    "The next step combines these two to create an update to the state. Pointwise multiplication operation (*) decides on the parts we output.\n",
    "\n",
    "$c_t = f_t * c_{t-1} + i_t * g_t$\n",
    "\n",
    "![lstm_it_cand](images/lstm_it_cand.png)\n",
    "\n",
    "Finally, the output gate decides how much information goes to the output:\n",
    "\n",
    "$o_t = \\sigma(W_{io}x_t + W_{ho}h_{t-1}+b_o)$\n",
    "\n",
    "$h_t = o_t * tanh(c_t)$\n",
    "\n",
    "![lstm_ot](images/lstm_ot.png)\n",
    "\n",
    "\n",
    "Q: How does this help with the vanishing gradient problem ?\n",
    "\n",
    "Whereas the RNN computes the new hidden state from scratch based on the previous hidden state and the input, the LSTM computes the new hidden state by choosing what to add to the current state. This allows skipping multiplicative gradient paths. Multiple additive paths are hard to converge to ~0.\n",
    "\n",
    "Q: How are language models trained ?\n",
    "\n",
    "Very often the so-called \"teacher forcing\" is used. It works by using the actual ground true outputs from the training dataset at the current time step t as input in the next time step t+1, rather than the output generated by the network. This makes learning faster and the model more stable. The model is not going to get punished for every subsequent word it generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manual_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        ## Code up the input gate\n",
    "        ## Code up the candidate gate\n",
    "        ## Code up the output gate\n",
    "        \n",
    "        \n",
    "        self.input_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.candidate_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "\n",
    "        # shape(x) = [B, T, input_size]\n",
    "        # shape(prev_hidden) = ([1, B, hidden_size], [1, B, hidden_size]) where 1 = num_layers * num_directions\n",
    "\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        \n",
    "        # At t=0, h_t and c_t will be initialized to a vector of 0s\n",
    "        h_t = prev_hidden[0].squeeze(0)\n",
    "        c_t = prev_hidden[1].squeeze(0)\n",
    "\n",
    "        hidden_states = torch.zeros(batch_size, sequence_length, self.hidden_size).to(device)\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            # shape(x_t) = [B, input_size]\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            # shape(concat_h_x) = [B, hidden_size+input_size]\n",
    "            concat_h_x = torch.cat((h_t, x_t), dim=-1)\n",
    "\n",
    "            # shape(f_t) = [B, hidden_size]\n",
    "            f_t = self.forget_gate(concat_h_x)\n",
    "\n",
    "            # shape(c_prime_t) = [B, hidden_size]\n",
    "            c_prime_t = c_t * f_t\n",
    "\n",
    "            # shape(i_t) = [B, hidden_size]\n",
    "            # shape(cand_t) = [B, hidden_size]\n",
    "            i_t = self.input_gate(concat_h_x)\n",
    "            cand_t = self.candidate_gate(concat_h_x)\n",
    "\n",
    "            # shape(c_t) = [B, hidden_size]\n",
    "            c_t = c_prime_t + (i_t * cand_t)\n",
    "\n",
    "            # shape(o_t) = [B, hidden_size]\n",
    "            # shape(h_t) = [B, hidden_size]\n",
    "            o_t = self.output_gate(concat_h_x)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_states[:, t, :] = h_t\n",
    "\n",
    "        h_t, c_t = h_t.unsqueeze(0), c_t.unsqueeze(0)\n",
    "        return hidden_states, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lstm = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"manual_lstm\")\n",
    "\n",
    "lstm = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(manual_lstm.parameters())\n",
    "train(manual_lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(lstm.parameters())\n",
    "train(lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the RNN, Manual LSTM and LSTM fare up against each other?\n",
    "\n",
    "![vanilla_rnn_lm](images/vanilla_rnn_lm.png)\n",
    "![manual_lstm_lm](images/manual_lstm_lm.png)\n",
    "![lstm_lm](images/lstm_lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs vs. GRUs\n",
    "\n",
    "Gated Recurrent Unit (GRU) combines the forget and input gates into a single \"update gate\" (z). So we have only two gates: update and reset. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models. Candidate state $g_t$ is able to suppress $h_t$. The final state is a convex combination: of the $g_t$ and $h_{t-1}$ with coefficients of $(1 - z_t)$ and $z_t$ respectively.\n",
    "\n",
    "$r_t = \\sigma(W_{ir}x_t + W_{hr}h_{t-1}+b_r)$\n",
    "\n",
    "$z_t = \\sigma(W_{iz}x_t + W_{hz}h_{t-1}+b_z)$\n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + r_t * (W_{hg}h_{t-1}+b_g))$\n",
    "\n",
    "$h_t = (1 - z_t)* g_t + z_t * h_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.gru = nn.GRU(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664biteff876fa6de145f1a297d7949827781d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
