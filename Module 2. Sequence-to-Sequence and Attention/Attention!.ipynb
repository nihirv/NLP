{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by [Zhenhao Li](mailto:zhenhao.li18@imperial.ac.uk), and [Nihir](mailto:nv419@ic.ac.uk).\n",
    "\n",
    "[Download the pretrained moedls](https://drive.google.com/drive/folders/1L-P-zSMe_pq-af97gXwnKKFsHRnulXAv?usp=sharing) (Only required for evaluation/testing if you don't want to train models from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Last week recap](#Last-week-recap)\n",
    "- [Bi-directional RNNs](#BiDirectional-RNNs)\n",
    "- [Sequence to sequence model](#Sequence-to-sequence-model)\n",
    " - [BLEU Score](#BLEU-Score)\n",
    "- [Attention](#Attention)\n",
    "- [Sequence to sequence with Attention](#Seq2seq-with-Attention)\n",
    "- [Transformer](#Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last week recap:\n",
    "\n",
    "![rnn_classification](images_lecture2/rnn_classification.png)\n",
    "![rnn_lm](images_lecture2/rnn_lm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import essential libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiDirectional RNNs\n",
    "\n",
    "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs at the same step are then usually concatenated. This can provide additional useful context to the model.\n",
    "\n",
    "Q: Why is a bi-directional RNN is better than single-direction ?\n",
    "\n",
    "Imagine that you see only the left context: \"We went to ...\" This context is very general and a lot of different words can continue: nouns (London, work, cinema, doctor), verbs (join, support), etc. When we both left and right contexts the word \"sleep\" is becoming evident: \"We went to ... early but still could not wake up on time.\"\n",
    "\n",
    "\n",
    "![bi_rnn_classification](images_lecture2/bi_rnn_classification.png)\n",
    "\n",
    "![](images_lecture2/bi_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence model\n",
    "\n",
    "https://arxiv.org/abs/1409.3215 \\\n",
    "So far we have encountered some classification tasks where the inputs are of variable length. We use Recurrent Neural Networks (RNN/LSTM/GRU) to do predictions. However, when it comes to text generation, the length of outputs might also be random. In this case, we use a sequence-to-sequence model. \\\n",
    "![](images_lecture2/seq2seq.png)\n",
    "\n",
    "A sequence-to-sequence (seq2seq) model is a model that consists of two components called **Encoder** and **Decoder**. Commonly, two recurrent neural networks are used as the encoder and the decoder. The input is fed into the encoder RNN token by token, producing a fix-lengthed vector (the final hidden state) that encodes the context of all input sequence. We refer to this vector as the **context vector**. The decoder uses this context vector as the initialization of its first hidden state and inits the input with the $<sos>$ token, generating the outputs token by token.\n",
    "\n",
    "Seq2seq model is often used in NLP tasks where the lengths of both input and output are not fixed, e.g. machine translation, dialogue system. In the following part, we are going to build a vanilla seq2seq model with LSTM as encoder/decoder module on the machine translation task.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html \\\n",
    "https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Encoder\n",
    "We have three layers in the encoder: an embedding layer (with dropout), a RNN layer, and a linear layer. As we have known from the word representation session, we can apply a embedding layer and distributed word representation is trained jointly with the model. \n",
    "\n",
    "If we want to have a bidirectional encoder to encode both forward and backward contexts in the input, the hidden dimension of the RNN layer is doubled. Therefore, the linear layer is here to keep the same dimensionality between the encoder output and decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, bidirectional=False, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "        \n",
    "        ## Initialise dropout\n",
    "        \n",
    "        \n",
    "        # If we use a bidirectional encoder to encode both forward and backward context,\n",
    "        # the dimension of the hidden state will double\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if bidirectional:\n",
    "            ff_input_dim = 2 * hidden_dim\n",
    "        else:\n",
    "            ff_input_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, self.hidden_dim, layers, dropout=dropout, \\\n",
    "                           bidirectional=bidirectional, bias=False, batch_first=True)\n",
    "        \n",
    "        # Our decoder will take something of shape [B, H].\n",
    "        # If we use bidirectional RNNs, we will be returning something of [B, 2H].\n",
    "         # So we apply a non-linearity to reduce our encoder dimensions to [B, H]\n",
    "        ## Apply a linear layer followed by a tanh activation\n",
    "        self.ff = nn.Sequential(\n",
    "           \n",
    "            \n",
    "        )\n",
    "        \n",
    "    # x: (T, B)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        x = x.permute(1, 0)\n",
    "        \n",
    "        ## embed the input, and then apply dropout\n",
    "        x = \n",
    "        # x: (B, T, E)\n",
    "\n",
    "        outputs, (h_n, c_n) = self.rnn(x)\n",
    "        # outputs: (B, T, H*directions)\n",
    "        # h_n: (layers*directions, B, H)\n",
    "        \n",
    "\n",
    "        if self.bidirectional:\n",
    "            # concatenate the forward and backward hidden states\n",
    "            h_n = torch.cat((h_n[0::2,:,:], h_n[1::2,:,:]), dim = -1)\n",
    "            c_n = torch.cat((c_n[0::2,:,:], c_n[1::2,:,:]), dim = -1)\n",
    "        \n",
    "        ## reduce dimensionality of our final hidden and cell state\n",
    "        h_n = \n",
    "        c_n = \n",
    "        # h_n: (layers, B, H)\n",
    "        # c_n: (layers, B, H)\n",
    "        \n",
    "        \n",
    "        return outputs, (h_n, c_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder has four layers: an embedding layer (with dropout), a unidirectional RNN layer and two linear layers. The decoder is always unidirectional in that we only generate the outputs from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        ## initialize dropout\n",
    "        \n",
    "        \n",
    "        ## initialize embedding\n",
    "        \n",
    "        \n",
    "        ## initialize LSTM (set batch_first=True)\n",
    "        # we DON'T set bidirectional=True here\n",
    "        self.rnn = nn.LSTM() # we don't set bidirectional here\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # x: (B)\n",
    "    def forward(self, x, hidden):\n",
    "        # we expand the dim of sequence length\n",
    "        x = x.unsqueeze(1)\n",
    "        # x: (B, 1)\n",
    "        \n",
    "        ## apply the embedding and dropout to x\n",
    "        embed = \n",
    "        # embed: (B, 1, E)\n",
    "        \n",
    "        ## run the LSTM\n",
    "        # We initialize the hidden & cell state of the decoder to the hidden & cell state of the encoder.\n",
    "         # Read https://pytorch.org/docs/stable/nn.html#lstm to see how this is done\n",
    "        ????\n",
    "        # output: (B, 1, H), h_n: (layers, B, H)\n",
    " \n",
    "        ## Run the output layer\n",
    "        output = \n",
    "        # output = (B, 1, O)\n",
    "        \n",
    "        ## Remove the dimension where there is \"1\"\n",
    "        output = \n",
    "        # output = (B, O)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Teacher forcing\n",
    "Teacher forcing is used in training to reduce error propagation and accelerate training. During training, the next input to the decoder can be either the ground truth token or the output by the decoder, determined by a *teacher_force_ratio*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device='cpu', with_attn=False):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.with_attn = with_attn\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, output_dim).to(self.device)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # initialize output sequence with '<sos>'\n",
    "        dec_output = trg[0,:]\n",
    "        print(\"DEC_OUTPUT\", dec_output.shape)\n",
    "        \n",
    "        # decoder token by token\n",
    "        for t in range(1, max_len):\n",
    "            if self.with_attn:\n",
    "                dec_output, hidden, _ = self.decoder(dec_output, hidden, enc_outputs)\n",
    "            else:\n",
    "                dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "                print(\"DEC_OUTPUT RNN\", dec_output.shape)\n",
    "                \n",
    "            outputs[t] = dec_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            pred_next = dec_output.argmax(1)\n",
    "            \n",
    "            dec_output = (trg[t] if teacher_force else pred_next)\n",
    "        return outputs\n",
    "\n",
    "    # greedy search for actual translation\n",
    "    def greedy_search(self, src, sos_idx, max_len=50, return_attention=False):\n",
    "        src = src.to(self.device)\n",
    "        batch_size = src.shape[1]\n",
    "        src_len = src.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size).to(self.device)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        \n",
    "        dec_output = torch.zeros(batch_size, dtype=torch.int64).to(device)\n",
    "        dec_output.fill_(sos_idx)\n",
    "        \n",
    "        outputs[0] = dec_output\n",
    "        \n",
    "        attentions = torch.zeros(max_len, batch_size, src_len).to(self.device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            if self.with_attn:\n",
    "                dec_output, hidden, attention_score = self.decoder(dec_output, hidden, enc_outputs)\n",
    "                attentions[t] = attention_score\n",
    "            else:\n",
    "                dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            \n",
    "            dec_output = dec_output.argmax(1)\n",
    "\n",
    "            outputs[t] = dec_output\n",
    "            \n",
    "        if return_attention:\n",
    "            return outputs, attentions\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished our seq2seq model, let's build a toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (embedding): Embedding(4, 10, padding_idx=1)\n",
      "    (rnn): LSTM(10, 6, num_layers=2, bias=False, batch_first=True, dropout=0.1)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (embedding): Embedding(4, 10, padding_idx=1)\n",
      "    (rnn): LSTM(10, 6, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=6, out_features=10, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (out): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM=4\n",
    "OUTPUT_DIM=4\n",
    "EMB_DIM=10\n",
    "HIDDEN_DIM=6\n",
    "LAYERS=2\n",
    "\n",
    "# define the encoder and decoder, and build the model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS)\n",
    "model = Seq2seq(enc, dec)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading with Torchtext\n",
    "https://pytorch.org/text/ \\\n",
    "Now we are running a machine translation model on actual dataset: Multi30k. Multi30k is a dataset for multi-modal machine translation. We'll only use the texts in this dataset and we load the dataset with *Torchtext*, which can help us with all the pre-processing and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# torchtext will pre-process the data, including tokenization, padding, stoi, etc.\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "# print the number of examples in train/valid/test sets\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab of our training set, ignoring word with frequency less than 2\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source example: ein mann mit einem orangefarbenen hut , der etwas anstarrt .\n",
      "Target example: a man in an orange hat starring at something .\n",
      "Padded target: [['<sos>', 'a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.', '<eos>']]\n",
      "Tensorized target: tensor([[   2],\n",
      "        [   4],\n",
      "        [   9],\n",
      "        [   6],\n",
      "        [  21],\n",
      "        [  86],\n",
      "        [  67],\n",
      "        [2599],\n",
      "        [  20],\n",
      "        [ 121],\n",
      "        [   5],\n",
      "        [   3]])\n"
     ]
    }
   ],
   "source": [
    "# build train/valid/test iterators, which will batch the data for us\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "x = vars(test_data.examples[0])['src']\n",
    "y = vars(test_data.examples[0])['trg']\n",
    "print(\"Source example:\", \" \".join(x))\n",
    "print(\"Target example:\", \" \".join(y))\n",
    "print(\"Padded target:\", TRG.pad([y]))\n",
    "print(\"Tensorized target:\", TRG.process([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, optimizer and criterion\n",
    "This is our model hyperparameters. In actual training, we might need to tune the hyperparameters on the validation set before evaluating on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM=256\n",
    "HIDDEN_DIM=512\n",
    "LAYERS=1\n",
    "DROPOUT=0.5\n",
    "BIDIRECTIONAL=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# padding token\n",
    "SRC_PAD = SRC.vocab.stoi['<pad>']\n",
    "TRG_PAD = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization sometimes boost training and the model can converge faster. We initialize the model parameters using a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(7855, 256, padding_idx=1)\n",
      "    (rnn): LSTM(256, 512, bias=False, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(5893, 256, padding_idx=1)\n",
      "    (rnn): LSTM(256, 512, batch_first=True, dropout=0.5)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (out): Linear(in_features=256, out_features=5893, bias=True)\n",
      "  )\n",
      ")\n",
      "The model has 10,412,805 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will update the gradient everytime we back-propagate. We are using Adam as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *CrossEntropyLoss* as our loss function, which will calculate the log softmax and the negative log-likelihood. We pass the padding token in the target vocab to the criterion so that it will ignore the loss for this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our training loop.\n",
    "1. We iterate over the training iterator and get a batch of training examples\n",
    "2. The input is passed through the model and it returns the predictions\n",
    "3. We calculate the loss between the model predictions and the ground truths\n",
    "4. We back-propagate the loss and the optimizer will update the gradients\n",
    "\n",
    "To avoid exploding gradient, we clip the gradients to a maximum value every training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip, num_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(src, trg)\n",
    "        \n",
    "        # exclude <sos> token\n",
    "        # outputs: (seq_len * batch_size, output_dim)\n",
    "        # trg : (seq_len * batch_size)\n",
    "        outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(outputs, trg)\n",
    "        \n",
    "        writer.add_scalar('training loss',\n",
    "                            loss.item(),\n",
    "                            num_epoch * len(iterator) + i)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Batch:\\t {0} / {1},\\t loss: {2:2.3f}'.format(i, len(iterator), loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        # clip grad to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluating loop is similar to the training loop, except that we don't want to do back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion):\n",
    "    # In eval model, layers such as Dropout, BatchNorm will work in eval model\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    # this prevents the back-propagation\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # during test time, we have no correct trg so we turn off teacher forcing\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "![](images_lecture2/bleu1.png)\n",
    "![](images_lecture2/bleu2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, converting a batch of tensors to the text form\n",
    "def get_text_from_tensor(tensor, field, eos='<eos>'):\n",
    "    batch_output = []\n",
    "    for i in range(tensor.shape[1]):\n",
    "        sequence = tensor[:,i]\n",
    "        words = []\n",
    "        for tok_idx in sequence:\n",
    "            tok_idx = int(tok_idx)\n",
    "            token = field.vocab.itos[tok_idx]\n",
    "\n",
    "            if token == '<sos>':\n",
    "                continue\n",
    "            elif token == '<eos>' or token == '<pad>':\n",
    "                break\n",
    "            else:\n",
    "                words.append(token)\n",
    "        words = \" \".join(words)\n",
    "        batch_output.append(words)\n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def test_bleu(model, iterator, trg_field, with_attention=False):\n",
    "    model.eval()\n",
    "\n",
    "    ref = []\n",
    "    hyp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            outputs = model.greedy_search(src, trg_field.vocab.stoi['<sos>'], return_attention=with_attention)\n",
    "            \n",
    "            hyp += get_text_from_tensor(outputs, trg_field)\n",
    "            ref += get_text_from_tensor(trg, trg_field)\n",
    "            \n",
    "    # expand dim of reference list\n",
    "    # sys = ['translation_1', 'translation_2']\n",
    "    # ref = [['truth_1', 'truth_2'], ['another truth_1', 'another truth_2']]\n",
    "    ref = [ref]\n",
    "    return sacrebleu.corpus_bleu(hyp, ref, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start our training! We keep the checkpoint with the highest valid BLEU as our best checkpoint.\n",
    "\n",
    "**The training is heavily dependent on GPU, so it might take years to train on CPU. You may skip this block and load our pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training Epoch 1:\n",
      "DEC_OUTPUT torch.Size([128])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b42e904cbb35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start training Epoch {}:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbleu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-83335d55abbd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, grad_clip, num_epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         writer.add_scalar('training loss',\n\u001b[0m\u001b[0;32m     23\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                             num_epoch * len(iterator) + i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = 0\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, i)\n",
    "    valid_loss = eval(model, valid_iterator, criterion)\n",
    "    bleu = test_bleu(model, valid_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set\n",
    "Finally we can evaluate our best model on the test set.\n",
    "Let's load the pre-trained model and calculate the bleu score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq.pt', map_location=torch.device(device)))\n",
    "print(test_bleu(model, test_iterator, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate\n",
    "Now we can translate an actual German sentence into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_position(tensor, field):\n",
    "    for position, tok_idx in enumerate(tensor):\n",
    "        tok_idx = int(tok_idx)\n",
    "        token = field.vocab.itos[tok_idx]\n",
    "        \n",
    "        if token == '<eos>' or token == '<pad>':\n",
    "            break\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, src_field, tgt_field):\n",
    "    tokens = src_field.preprocess(text)\n",
    "    input_tensor = src_field.process([tokens])\n",
    "    \n",
    "    outputs = model.greedy_search(input_tensor, tgt_field.vocab.stoi['<sos>'])\n",
    "    \n",
    "    output_text = get_text_from_tensor(outputs, tgt_field)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'ein mann mit einem orangefarbenen hut, der etwas anstarrt.', SRC, TRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem with simple Seq2seq model\n",
    "The seq2seq model we learned compress the whole input sentence into a fixed-length vector. However, there might be a problem when the input sequence become very long. It is hard to compress all necessary information in this fixed-length context vector, and the final hidden states tend to include recent words information and forget words with long distance. \n",
    "\n",
    "Rather than using a single context vector, we can use all the encoder hidden states, and now we are introducing the attention mechanism.\n",
    "\n",
    "# Attention\n",
    "https://arxiv.org/pdf/1409.0473.pdf\n",
    "https://arxiv.org/pdf/1508.04025.pdf\n",
    "The attention mechanism allows the decoder to search the source sentence and concentrate on the more relevant information (based on previous decoder hidden state), at each decoding step. \\\n",
    "At decoding step $t$, a context vector $c_i$ is computed as a weighted sum of all encoder hidden states.\n",
    "$$ c_i = \\sum^T_t\\alpha_{i,t}h_t$$\n",
    "The weight $\\alpha_{i,t}$ represent the relatedness of source token at position $t$ when the decoder is generating the target token $i$.\n",
    "Then we need to calculate the relatedness by a certain function (alignment function). $$e_{i,t}=a(s_{i-1}, h_t)$$\n",
    "To have the weights summing up to 1, we can apply a softmax over $e$, and we can have $$\\alpha_{i,t}=\\frac{exp(e_{i,t})}{\\sum_{k=1}^Texp(e_{i,k})}$$\n",
    "### Alignment function\n",
    "The alignment function can be of various types. In the original attention paper, a MLP is used: $$a(s_{i-1}, h_t) = v_a^Ttanh(W_as_{i-1}+U_ah_t)$$\n",
    "\n",
    "![](https://insidebigdata.com/wp-content/uploads/2018/09/Eleks_3.png)\n",
    "![](images/seq2seq_attention.png)\n",
    "![](images/attention_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq with Attention\n",
    "In the following parts, we are going to implement a sequence-to-sequence model with the attention mechanism and compare it with the previous seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # Why is self.W 3*H?\n",
    "        self.W = nn.Linear(3*hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    # dec_hidden： （1, B, H)\n",
    "    # enc_outputs: (B, T, 2H)\n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        \n",
    "        src_len = enc_outputs.shape[1] #T\n",
    "        \n",
    "        dec_hidden = dec_hidden.permute(1, 0, 2)\n",
    "        # dec_hidden: (B, 1, H)\n",
    "\n",
    "        dec_hidden = dec_hidden.repeat(1, src_len, 1)\n",
    "        # dec_hidden: (B, T, H)\n",
    "        \n",
    "        energy = F.tanh(self.W(torch.cat((dec_hidden, enc_outputs), dim=-1)))\n",
    "        # energy: (B, T, H)\n",
    "        \n",
    "        attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        # attention: (B, T)\n",
    "\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, PAD_IDX=1, dropout=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        ## initialize dropout\n",
    "        self.dropout = \n",
    "        \n",
    "        ## initialize embedding\n",
    "        self.embedding = \n",
    "        \n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # Why is the input size to this LSTM emb_dim + 2*hidden_dim?\n",
    "        self.rnn = nn.LSTM(emb_dim+2*hidden_dim, hidden_dim, dropout=dropout, batch_first=True) \n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3*hidden_dim + emb_dim, output_dim),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # x: (B)\n",
    "    # hidden=(h_n, c_n): (1, B, H)\n",
    "    # enc_outputs=(B, T, 2H)\n",
    "    def forward(self, x, hidden, enc_outputs):\n",
    "        # we expand the dim of sequence length\n",
    "        # x: (B, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        ## run dropout and embedding\n",
    "        embed = \n",
    "        # embed: (B, 1, E)\n",
    "        \n",
    "        attn_score = self.attention(hidden[0], enc_outputs).unsqueeze(1)\n",
    "        # attn_score: (B, 1, T)\n",
    "        \n",
    "        c = torch.bmm(attn_score, enc_outputs)\n",
    "        # c: (B, 1, 2H)\n",
    "        \n",
    "        output, hidden = self.rnn(torch.cat((embed, c), dim=2), hidden)\n",
    "        # output: (B, 1, H)\n",
    "        # h_n: (1, B, H)\n",
    "        \n",
    "        ## Concatenate relevant inputs and feed this to the output layer\n",
    "        output = \n",
    "        # output: (B, 1, O)\n",
    "        \n",
    "        ## Squeeze the output\n",
    "        output = \n",
    "        # output: (B, output_dim)\n",
    "\n",
    "        return output, hidden, attn_score.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "We use the same hyperparameters for the attention-based model, but we need to reset the optimizer and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding token\n",
    "SRC_PAD = SRC.vocab.stoi['<pad>']\n",
    "TRG_PAD = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = AttnDecoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device, with_attn=True).to(device)\n",
    "\n",
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = 0\n",
    "writer = SummaryWriter('runs/seq2seq-attn')\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, i)\n",
    "    valid_loss = eval(model, valid_iterator, criterion)\n",
    "    bleu = test_bleu(model, valid_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq-attn.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq-attn.pt', map_location=torch.device(device)))\n",
    "print(test_bleu(model, test_iterator, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate\n",
    "Now we can translate an actual German sentence into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# helper function: plot attention heatmap with plotly\n",
    "def plot_attention(src_text, trg_text, attentions):\n",
    "    layout = go.Layout(\n",
    "    title=\"<b>Heatmap</b>\",\n",
    "    xaxis={\"mirror\" : \"allticks\", 'side': 'top'}, \n",
    "    yaxis={\"mirror\" : \"allticks\", 'side': 'left'}  \n",
    "    )\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                   z=attentions.cpu().numpy(),\n",
    "                   x=src_text,\n",
    "                   y=trg_text,\n",
    "                   hoverongaps = False), layout=layout)\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, src_field, tgt_field):\n",
    "    tokens = src_field.preprocess(text)\n",
    "    input_tensor = src_field.process([tokens])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, attention = model.greedy_search(input_tensor, tgt_field.vocab.stoi['<sos>'], return_attention=True)\n",
    "        eos_position = get_eos_position(outputs.squeeze(1), TRG)    \n",
    "        output_text = get_text_from_tensor(outputs, tgt_field)\n",
    "    \n",
    "    # valid_attention: (trg_len, src_len)\n",
    "    valid_attention = attention[1:eos_position+1,:,:].squeeze(1)\n",
    "    src = ['<sos>'] + tokens + ['<eos>']\n",
    "    trg = output_text[0].split() + ['<eos>']\n",
    "    plot_attention(src, trg, valid_attention)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'ein mann mit einem orangefarbenen hut, der etwas anstarrt.', SRC, TRG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
